# Story 0.3 TDD å¼€å‘è®¡åˆ’ï¼šå›å½’æµ‹è¯•å’Œæ€§èƒ½éªŒè¯

**Story ID**: STORY-0.3
**Epic**: Epic-001 (Phase 0: å·¥å…·è°ƒç”¨é“¾è·¯ç®€åŒ–)
**å·¥æœŸ**: 2 äººå¤©
**é£é™©**: ğŸŸ¢ ä½
**ä¾èµ–**: Story 0.1 + Story 0.2 å®Œæˆ
**çŠ¶æ€**: [PLANNING]

---

## ğŸ“‹ Story ç›®æ ‡

é€šè¿‡å…¨é¢çš„å›å½’æµ‹è¯•éªŒè¯ç®€åŒ–åçš„è°ƒç”¨é“¾è·¯æ²¡æœ‰ç ´åç°æœ‰åŠŸèƒ½ï¼Œå¹¶é€šè¿‡æ€§èƒ½æµ‹è¯•éªŒè¯ä¼˜åŒ–æ•ˆæœã€‚

**äº¤ä»˜ç‰©**ï¼š
1. æ‰€æœ‰ç°æœ‰æµ‹è¯•é€šè¿‡ï¼ˆengine enabled + disabled ä¸¤ç§æ¨¡å¼ï¼‰
2. å®¡è®¡æ—¥å¿—æ­£ç¡®æ€§éªŒè¯
3. æ€§èƒ½åŸºå‡†æµ‹è¯•å’Œå¯¹æ¯”
4. Phase 0 å®ŒæˆæŠ¥å‘Š

---

## ğŸ¯ éªŒè¯èŒƒå›´

### 1. åŠŸèƒ½æ­£ç¡®æ€§éªŒè¯

- âœ… æ‰€æœ‰å·¥å…·æ­£å¸¸å·¥ä½œï¼ˆ25+ languagesï¼‰
- âœ… å·¥å…·è°ƒç”¨è¡Œä¸ºä¸å˜
- âœ… å¼‚å¸¸å¤„ç†æ­£ç¡®
- âœ… æ—¥å¿—è®°å½•å®Œæ•´

### 2. å®¡è®¡èƒ½åŠ›éªŒè¯

- âœ… ExecutionContext æ­£ç¡®è®°å½•
- âœ… å®¡è®¡æ—¥å¿—å®Œæ•´æ€§
- âœ… Token è¿½è¸ªå‡†ç¡®æ€§
- âœ… TPST åˆ†ææ¥å£å¯ç”¨

### 3. æ€§èƒ½æŒ‡æ ‡éªŒè¯

- âœ… æ€§èƒ½å½±å“ < 5%
- âœ… æ— å†…å­˜æ³„æ¼
- âœ… çº¿ç¨‹æ± æ­£å¸¸å·¥ä½œ
- âœ… å“åº”æ—¶é—´ç¨³å®š

---

## ğŸ§ª TDD å¼€å‘ç­–ç•¥

### éªŒè¯å¾ªç¯è®¡åˆ’

é‡‡ç”¨**åˆ†å±‚éªŒè¯**ç­–ç•¥ï¼Œä»å•å…ƒåˆ°é›†æˆåˆ°æ€§èƒ½ï¼š

```
Cycle 1: ç°æœ‰å•å…ƒæµ‹è¯•å›å½’
  éªŒè¯æ‰€æœ‰å•å…ƒæµ‹è¯•é€šè¿‡ï¼ˆengine enabled + disabledï¼‰

Cycle 2: ç°æœ‰é›†æˆæµ‹è¯•å›å½’
  éªŒè¯æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡

Cycle 3: å®¡è®¡æ—¥å¿—éªŒè¯
  éªŒè¯å®¡è®¡åŠŸèƒ½å®Œæ•´æ€§

Cycle 4: æ€§èƒ½åŸºå‡†æµ‹è¯•
  å¯¹æ¯”ä¼˜åŒ–å‰åæ€§èƒ½

Cycle 5: é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•
  éªŒè¯å†…å­˜å’Œçº¿ç¨‹ç¨³å®šæ€§
```

---

## ğŸ“ æµ‹è¯•æ–‡ä»¶ç»“æ„

```
test/evolvai/
â””â”€â”€ phase0_validation/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_regression_unit.py         # å•å…ƒæµ‹è¯•å›å½’
    â”œâ”€â”€ test_regression_integration.py  # é›†æˆæµ‹è¯•å›å½’
    â”œâ”€â”€ test_audit_validation.py        # å®¡è®¡æ—¥å¿—éªŒè¯
    â”œâ”€â”€ test_performance_baseline.py    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    â””â”€â”€ test_stability.py               # ç¨³å®šæ€§æµ‹è¯•

docs/development/sprints/current/
â””â”€â”€ phase-0-completion-report.md       # å®ŒæˆæŠ¥å‘Š
```

---

## ğŸ”´ Cycle 1: ç°æœ‰å•å…ƒæµ‹è¯•å›å½’

### éªŒè¯ç­–ç•¥

è¿è¡Œæ‰€æœ‰ç°æœ‰å•å…ƒæµ‹è¯•ï¼Œåˆ†åˆ«åœ¨ä¸¤ç§æ¨¡å¼ä¸‹ï¼š
1. **Engine Enabled**: `enable_execution_engine=True`
2. **Engine Disabled**: `enable_execution_engine=False`

### æµ‹è¯•æ–‡ä»¶

**æµ‹è¯•æ–‡ä»¶**: `test/evolvai/phase0_validation/test_regression_unit.py`

```python
"""Regression tests for existing unit tests with ToolExecutionEngine."""
import pytest

from serena.config import SerenaConfig


@pytest.fixture(params=[True, False], ids=["engine_enabled", "engine_disabled"])
def execution_engine_mode(request, monkeypatch):
    """Parametrize tests to run with engine enabled and disabled."""
    enable_engine = request.param

    # Monkey-patch SerenaConfig default
    original_init = SerenaConfig.__init__

    def patched_init(self, *args, **kwargs):
        original_init(self, *args, **kwargs)
        self.enable_execution_engine = enable_engine

    monkeypatch.setattr(SerenaConfig, "__init__", patched_init)

    return enable_engine


class TestUnitTestsRegression:
    """Verify all unit tests pass in both engine modes."""

    def test_existing_tool_tests_pass(self, execution_engine_mode):
        """Test that all existing tool unit tests pass."""
        # This is a marker test - actual verification done by pytest
        # running existing test suite with the fixture
        assert True  # Will fail if any existing test fails

    def test_tool_parameter_validation_unchanged(self, execution_engine_mode):
        """Test that tool parameter validation behavior unchanged."""
        # Run test/serena/test_tool_parameter_types.py
        pass

    def test_tool_execution_behavior_identical(self, execution_engine_mode):
        """Test that tool execution behavior is identical."""
        # Compare results between engine enabled/disabled for same inputs
        pass
```

### æ‰§è¡Œå‘½ä»¤

```bash
# Run all existing tests with both modes
pytest test/serena/ -v --log-cli-level=INFO

# Specifically test with engine enabled
pytest test/serena/ -v -k "engine_enabled"

# Specifically test with engine disabled
pytest test/serena/ -v -k "engine_disabled"
```

### éªŒæ”¶æ ‡å‡†

- âœ… **100% æµ‹è¯•é€šè¿‡ç‡**ï¼ˆengine enabledï¼‰
- âœ… **100% æµ‹è¯•é€šè¿‡ç‡**ï¼ˆengine disabledï¼‰
- âœ… æ— æ–°çš„è­¦å‘Šæˆ–é”™è¯¯
- âœ… æµ‹è¯•æ‰§è¡Œæ—¶é—´å˜åŒ– < 5%

---

## ğŸ”´ Cycle 2: ç°æœ‰é›†æˆæµ‹è¯•å›å½’

### éªŒè¯ç­–ç•¥

è¿è¡Œæ‰€æœ‰é›†æˆæµ‹è¯•ï¼ŒéªŒè¯å¤šå·¥å…·åä½œã€MCP é›†æˆã€è¯­è¨€æœåŠ¡å™¨é›†æˆç­‰å¤æ‚åœºæ™¯ã€‚

### æµ‹è¯•æ–‡ä»¶

**æµ‹è¯•æ–‡ä»¶**: `test/evolvai/phase0_validation/test_regression_integration.py`

```python
"""Regression tests for integration scenarios."""
import pytest

from serena.agent import SerenaAgent


class TestIntegrationRegression:
    """Verify integration scenarios work correctly."""

    def test_serena_agent_initialization(self, tmp_path):
        """Test SerenaAgent initializes correctly with engine."""
        config = SerenaConfig(serena_home=tmp_path)
        agent = SerenaAgent(serena_config=config)

        assert agent is not None
        assert agent._execution_engine is not None

    def test_mcp_server_integration(self, tmp_path):
        """Test MCP server works with ToolExecutionEngine."""
        # Test that MCP server can start and handle requests
        pass

    def test_multi_language_support(self, tmp_path):
        """Test that all language servers work correctly."""
        # Run symbolic editing tests for multiple languages
        pass

    def test_tool_chaining(self, tmp_path):
        """Test that multiple tools can be chained."""
        # Test complex workflows involving multiple tool calls
        pass

    def test_context_mode_switching(self, tmp_path):
        """Test that context and mode switching still works."""
        # Test SerenaAgent's Context & Mode system
        pass

    def test_memory_system_integration(self, tmp_path):
        """Test that memory tools work correctly."""
        # Test write_memory, read_memory, etc.
        pass
```

### å…³é”®é›†æˆåœºæ™¯

1. **MCP æœåŠ¡å™¨å¯åŠ¨å’Œå“åº”**
   ```bash
   uv run serena-mcp-server
   # éªŒè¯æœåŠ¡å™¨æ­£å¸¸å¯åŠ¨ï¼Œæ— é”™è¯¯
   ```

2. **ç¬¦å·ç¼–è¾‘æ“ä½œ**ï¼ˆå¤šè¯­è¨€ï¼‰
   ```bash
   pytest test/serena/test_symbol_editing.py -v
   ```

3. **å·¥å…·é“¾åä½œ**
   ```bash
   pytest test/serena/test_serena_agent.py -v
   ```

### éªŒæ”¶æ ‡å‡†

- âœ… æ‰€æœ‰é›†æˆæµ‹è¯•é€šè¿‡
- âœ… MCP æœåŠ¡å™¨æ­£å¸¸å¯åŠ¨
- âœ… å¤šè¯­è¨€æ”¯æŒæ­£å¸¸
- âœ… Context & Mode ç³»ç»Ÿæ­£å¸¸

---

## ğŸ”´ Cycle 3: å®¡è®¡æ—¥å¿—éªŒè¯

### éªŒè¯ç›®æ ‡

éªŒè¯ ToolExecutionEngine çš„å®¡è®¡åŠŸèƒ½æ­£ç¡®å·¥ä½œï¼Œä¸º TPST åˆ†ææä¾›åŸºç¡€ã€‚

### æµ‹è¯•æ–‡ä»¶

**æµ‹è¯•æ–‡ä»¶**: `test/evolvai/phase0_validation/test_audit_validation.py`

```python
"""Tests for audit log validation."""
import pytest

from serena.agent import SerenaAgent
from serena.tools.tools_base import Tool


class MockAuditTool(Tool):
    """Tool for testing audit functionality."""

    def apply(self, test_input: str) -> str:
        """Mock apply.

        :param test_input: Test input
        :return: Result
        """
        return f"processed: {test_input}"


class TestAuditLogValidation:
    """Validate audit log functionality."""

    @pytest.fixture
    def agent(self, tmp_path):
        """Create agent with audit enabled."""
        config = SerenaConfig(serena_home=tmp_path)
        return SerenaAgent(serena_config=config)

    def test_audit_log_records_all_executions(self, agent):
        """Test that audit log records all tool executions."""
        tool = MockAuditTool(agent)

        # Execute tool multiple times
        tool.apply_ex(test_input="test1")
        tool.apply_ex(test_input="test2")
        tool.apply_ex(test_input="test3")

        # Check audit log
        audit_log = agent._execution_engine._audit_log
        assert len(audit_log) == 3

    def test_audit_record_contains_required_fields(self, agent):
        """Test that audit records contain all required fields."""
        tool = MockAuditTool(agent)
        tool.apply_ex(test_input="test")

        record = agent._execution_engine._audit_log[0]

        # Verify required fields
        assert "tool" in record
        assert "phase" in record
        assert "duration" in record
        assert "tokens" in record
        assert "success" in record
        assert "constraints" in record
        assert "batched" in record

    def test_audit_log_captures_success_and_failure(self, agent):
        """Test that audit log correctly captures success and failure."""
        # Success case
        tool = MockAuditTool(agent)
        tool.apply_ex(test_input="test")

        success_record = agent._execution_engine._audit_log[-1]
        assert success_record["success"] is True

        # Failure case (if tool raises exception)
        # ... test error scenario

    def test_audit_log_duration_accurate(self, agent):
        """Test that duration tracking is accurate."""
        import time

        tool = MockAuditTool(agent)

        start = time.time()
        tool.apply_ex(test_input="test")
        end = time.time()

        actual_duration = end - start
        recorded_duration = agent._execution_engine._audit_log[-1]["duration"]

        # Duration should be within reasonable range
        assert 0 < recorded_duration <= actual_duration + 0.1

    def test_audit_log_can_be_queried(self, agent):
        """Test that audit log can be queried and analyzed."""
        tool = MockAuditTool(agent)

        # Execute multiple times
        for i in range(10):
            tool.apply_ex(test_input=f"test{i}")

        # Query audit log
        audit_log = agent._execution_engine._audit_log

        # Should be able to filter, aggregate, etc.
        successful_executions = [r for r in audit_log if r["success"]]
        assert len(successful_executions) == 10

    def test_audit_log_supports_tpst_analysis(self, agent):
        """Test that audit log supports TPST analysis."""
        tool = MockAuditTool(agent)

        tool.apply_ex(test_input="test")

        # Should be able to extract TPST metrics
        total_tokens = sum(r["tokens"] for r in agent._execution_engine._audit_log)
        total_successful = sum(1 for r in agent._execution_engine._audit_log if r["success"])

        # Basic TPST calculation
        if total_successful > 0:
            tpst = total_tokens / total_successful
            assert tpst >= 0
```

### éªŒæ”¶æ ‡å‡†

- âœ… å®¡è®¡æ—¥å¿—è®°å½•æ‰€æœ‰æ‰§è¡Œ
- âœ… å®¡è®¡è®°å½•åŒ…å«å®Œæ•´å­—æ®µ
- âœ… æ—¶é—´è¿½è¸ªå‡†ç¡®
- âœ… æ”¯æŒ TPST åˆ†æ
- âœ… å®¡è®¡æ—¥å¿—å¯æŸ¥è¯¢å’Œåˆ†æ

---

## ğŸ”´ Cycle 4: æ€§èƒ½åŸºå‡†æµ‹è¯•

### æµ‹è¯•ç›®æ ‡

å¯¹æ¯”ä¼˜åŒ–å‰åçš„æ€§èƒ½ï¼ŒéªŒè¯æ€§èƒ½å½±å“ < 5% çš„ç›®æ ‡ã€‚

### æµ‹è¯•æ–‡ä»¶

**æµ‹è¯•æ–‡ä»¶**: `test/evolvai/phase0_validation/test_performance_baseline.py`

```python
"""Performance baseline tests for Phase 0."""
import time
import statistics

import pytest

from serena.agent import SerenaAgent
from serena.tools.tools_base import Tool


class BenchmarkTool(Tool):
    """Tool for performance benchmarking."""

    def apply(self, iterations: int = 100) -> str:
        """Simple operation for benchmarking.

        :param iterations: Number of iterations
        :return: Result
        """
        result = 0
        for i in range(iterations):
            result += i
        return f"completed {iterations} iterations"


class TestPerformanceBaseline:
    """Performance baseline tests."""

    @pytest.fixture
    def agent_with_engine(self, tmp_path):
        """Agent with execution engine enabled."""
        config = SerenaConfig(
            serena_home=tmp_path,
            enable_execution_engine=True
        )
        return SerenaAgent(serena_config=config)

    @pytest.fixture
    def agent_without_engine(self, tmp_path):
        """Agent with execution engine disabled (legacy)."""
        config = SerenaConfig(
            serena_home=tmp_path,
            enable_execution_engine=False
        )
        return SerenaAgent(serena_config=config)

    def benchmark_tool_execution(self, agent, iterations=100):
        """Benchmark tool execution time."""
        tool = BenchmarkTool(agent)

        execution_times = []
        for _ in range(iterations):
            start = time.perf_counter()
            tool.apply_ex(iterations=10)
            end = time.perf_counter()
            execution_times.append(end - start)

        return {
            "mean": statistics.mean(execution_times),
            "median": statistics.median(execution_times),
            "stdev": statistics.stdev(execution_times) if len(execution_times) > 1 else 0,
            "min": min(execution_times),
            "max": max(execution_times),
        }

    def test_performance_impact_within_threshold(
        self,
        agent_with_engine,
        agent_without_engine
    ):
        """Test that performance impact is < 5%."""
        # Benchmark with engine
        with_engine_stats = self.benchmark_tool_execution(agent_with_engine, iterations=50)

        # Benchmark without engine (legacy)
        without_engine_stats = self.benchmark_tool_execution(agent_without_engine, iterations=50)

        # Calculate performance impact
        mean_with = with_engine_stats["mean"]
        mean_without = without_engine_stats["mean"]

        performance_impact = ((mean_with - mean_without) / mean_without) * 100

        print(f"\nPerformance Impact: {performance_impact:.2f}%")
        print(f"With Engine: {mean_with*1000:.3f}ms (Â±{with_engine_stats['stdev']*1000:.3f}ms)")
        print(f"Without Engine: {mean_without*1000:.3f}ms (Â±{without_engine_stats['stdev']*1000:.3f}ms)")

        # Verify performance impact < 5%
        assert performance_impact < 5.0, f"Performance impact {performance_impact:.2f}% exceeds 5% threshold"

    def test_memory_usage_stable(self, agent_with_engine):
        """Test that memory usage is stable over time."""
        import tracemalloc

        tracemalloc.start()

        tool = BenchmarkTool(agent_with_engine)

        # Take initial memory snapshot
        snapshot1 = tracemalloc.take_snapshot()

        # Execute many times
        for _ in range(1000):
            tool.apply_ex(iterations=10)

        # Take final memory snapshot
        snapshot2 = tracemalloc.take_snapshot()

        # Compare memory usage
        top_stats = snapshot2.compare_to(snapshot1, 'lineno')

        # Memory increase should be minimal
        total_increase = sum(stat.size_diff for stat in top_stats)

        print(f"\nTotal memory increase: {total_increase / 1024 / 1024:.2f} MB")

        # Should not leak significant memory (< 10MB for 1000 executions)
        assert total_increase < 10 * 1024 * 1024

        tracemalloc.stop()

    def test_response_time_consistent(self, agent_with_engine):
        """Test that response times are consistent."""
        tool = BenchmarkTool(agent_with_engine)

        execution_times = []
        for _ in range(100):
            start = time.perf_counter()
            tool.apply_ex(iterations=10)
            end = time.perf_counter()
            execution_times.append(end - start)

        # Calculate coefficient of variation (CV)
        mean_time = statistics.mean(execution_times)
        stdev_time = statistics.stdev(execution_times)
        cv = (stdev_time / mean_time) * 100

        print(f"\nResponse Time CV: {cv:.2f}%")

        # CV should be < 20% (indicating consistent performance)
        assert cv < 20.0

    @pytest.mark.slow
    def test_long_running_stability(self, agent_with_engine):
        """Test stability over extended period."""
        tool = BenchmarkTool(agent_with_engine)

        # Run for extended period
        start_time = time.time()
        execution_count = 0

        while time.time() - start_time < 60:  # Run for 1 minute
            tool.apply_ex(iterations=10)
            execution_count += 1

        # Should complete many executions without errors
        assert execution_count > 100

        # Audit log should be populated
        assert len(agent_with_engine._execution_engine._audit_log) == execution_count
```

### æ€§èƒ½æŒ‡æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ | æµ‹é‡æ–¹æ³• |
|------|------|---------|
| æ‰§è¡Œæ—¶é—´å¼€é”€ | < 5% | å¯¹æ¯” engine enabled vs disabled |
| å†…å­˜ç¨³å®šæ€§ | < 10MB/1000æ¬¡ | tracemalloc è¿½è¸ª |
| å“åº”æ—¶é—´ä¸€è‡´æ€§ | CV < 20% | 100 æ¬¡æ‰§è¡Œçš„å˜å¼‚ç³»æ•° |
| é•¿æœŸç¨³å®šæ€§ | æ— é”™è¯¯ | 60s è¿ç»­æ‰§è¡Œ |

### éªŒæ”¶æ ‡å‡†

- âœ… æ€§èƒ½å½±å“ < 5%
- âœ… æ— å†…å­˜æ³„æ¼
- âœ… å“åº”æ—¶é—´ç¨³å®š
- âœ… é•¿æœŸè¿è¡Œç¨³å®š

---

## ğŸ”´ Cycle 5: é•¿æ—¶é—´ç¨³å®šæ€§æµ‹è¯•

### æµ‹è¯•æ–‡ä»¶

**æµ‹è¯•æ–‡ä»¶**: `test/evolvai/phase0_validation/test_stability.py`

```python
"""Long-term stability tests."""
import pytest


@pytest.mark.slow
@pytest.mark.stability
class TestLongTermStability:
    """Test long-term stability of ToolExecutionEngine."""

    def test_extended_operation(self, agent_with_engine):
        """Test extended operation without degradation."""
        # Run many operations over extended period
        # Monitor for memory leaks, performance degradation, errors
        pass

    def test_audit_log_growth_manageable(self, agent_with_engine):
        """Test that audit log growth is manageable."""
        # Execute many operations
        # Verify audit log doesn't grow unbounded
        # (Future: implement log rotation/pruning)
        pass

    def test_concurrent_tool_execution(self, agent_with_engine):
        """Test concurrent tool execution stability."""
        # Test thread pool handling
        # Verify no race conditions or deadlocks
        pass
```

---

## ğŸ“Š Phase 0 å®ŒæˆæŠ¥å‘Š

### æŠ¥å‘Šæ¨¡æ¿

**æ–‡ä»¶**: `docs/development/sprints/current/phase-0-completion-report.md`

```markdown
# Phase 0 å®ŒæˆæŠ¥å‘Š

**å®Œæˆæ—¥æœŸ**: [æ—¥æœŸ]
**å¼€å‘å‘¨æœŸ**: 10 äººå¤©
**çŠ¶æ€**: âœ… å®Œæˆ

## äº¤ä»˜ç‰©æ¸…å•

- [x] Story 0.1: ToolExecutionEngine å®ç°
- [x] Story 0.2: SerenaAgent é›†æˆ
- [x] Story 0.3: å›å½’æµ‹è¯•å’Œæ€§èƒ½éªŒè¯

## æµ‹è¯•ç»“æœ

### åŠŸèƒ½æµ‹è¯•
- å•å…ƒæµ‹è¯•é€šè¿‡ç‡: 100% (xxx/xxx tests)
- é›†æˆæµ‹è¯•é€šè¿‡ç‡: 100% (xxx/xxx tests)
- æµ‹è¯•è¦†ç›–ç‡: xx%

### å®¡è®¡èƒ½åŠ›éªŒè¯
- âœ… ExecutionContext è®°å½•å®Œæ•´
- âœ… å®¡è®¡æ—¥å¿—å¯æŸ¥è¯¢
- âœ… TPST åˆ†ææ¥å£å¯ç”¨

### æ€§èƒ½æµ‹è¯•
- æ‰§è¡Œæ—¶é—´å½±å“: x.x% (< 5% âœ…)
- å†…å­˜ç¨³å®šæ€§: âœ… æ— æ³„æ¼
- å“åº”æ—¶é—´: ç¨³å®š (CV = x.x%)

## æ¶æ„å˜åŒ–ç¡®è®¤

- âœ… è°ƒç”¨é“¾è·¯: 7 å±‚ â†’ 4 å±‚
- âœ… ç»Ÿä¸€æ‰§è¡Œå…¥å£: ToolExecutionEngine
- âœ… å®Œæ•´å®¡è®¡: ExecutionContext
- âœ… Feature flags: enable_execution_engine, enable_constraints

## é—ç•™é—®é¢˜

[å¦‚æœ‰]

## ä¸‹ä¸€æ­¥

- [ ] Phase 1: ExecutionPlan éªŒè¯æ¡†æ¶
- [ ] Feature 1.1: ExecutionPlan Schema
- [ ] ...

---

**æ‰¹å‡†äºº**: EvolvAI Team
**æ‰¹å‡†æ—¥æœŸ**: [æ—¥æœŸ]
```

---

## âœ… Definition of Done

### æµ‹è¯•éªŒæ”¶

- [ ] **100%** ç°æœ‰å•å…ƒæµ‹è¯•é€šè¿‡ï¼ˆengine enabledï¼‰
- [ ] **100%** ç°æœ‰å•å…ƒæµ‹è¯•é€šè¿‡ï¼ˆengine disabledï¼‰
- [ ] **100%** é›†æˆæµ‹è¯•é€šè¿‡
- [ ] **æ— ** æ–°å¢è­¦å‘Šæˆ–é”™è¯¯

### æ€§èƒ½éªŒæ”¶

- [ ] æ€§èƒ½å½±å“ **< 5%**
- [ ] æ— å†…å­˜æ³„æ¼ï¼ˆ< 10MB/1000æ¬¡ï¼‰
- [ ] å“åº”æ—¶é—´ç¨³å®šï¼ˆCV < 20%ï¼‰
- [ ] é•¿æœŸç¨³å®šæ€§éªŒè¯é€šè¿‡

### å®¡è®¡éªŒæ”¶

- [ ] å®¡è®¡æ—¥å¿—è®°å½• **100%** æ‰§è¡Œ
- [ ] å®¡è®¡è®°å½•åŒ…å«æ‰€æœ‰å¿…éœ€å­—æ®µ
- [ ] æ—¶é—´è¿½è¸ªè¯¯å·® **< 100ms**
- [ ] TPST åˆ†ææ¥å£å¯ç”¨

### æ–‡æ¡£éªŒæ”¶

- [ ] Phase 0 å®ŒæˆæŠ¥å‘Šå·²åˆ›å»º
- [ ] æ€§èƒ½åŸºå‡†æ•°æ®å·²è®°å½•
- [ ] å·²çŸ¥é—®é¢˜å·²æ–‡æ¡£åŒ–
- [ ] å‘ Epic-001 README ç¡®è®¤ Phase 0 å®Œæˆ

---

## ğŸ“Š æ¯æ—¥è¿›åº¦è·Ÿè¸ª

### Day 1: Cycle 1-2 (å›å½’æµ‹è¯•)
- âœ… Cycle 1: å•å…ƒæµ‹è¯•å›å½’
- âœ… Cycle 2: é›†æˆæµ‹è¯•å›å½’

### Day 2: Cycle 3-5 + æŠ¥å‘Š
- â³ Cycle 3: å®¡è®¡æ—¥å¿—éªŒè¯
- â³ Cycle 4: æ€§èƒ½åŸºå‡†æµ‹è¯•
- â³ Cycle 5: ç¨³å®šæ€§æµ‹è¯•
- â³ Phase 0 å®ŒæˆæŠ¥å‘Š

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [Story 0.1: å®ç° ToolExecutionEngine](./story-0.1-tdd-plan.md)
- [Story 0.2: é›†æˆåˆ° SerenaAgent](./story-0.2-tdd-plan.md)
- [Phase 0 è¯¦ç»†è®¾è®¡](../../architecture/phase-0-tool-execution-engine.md)
- [Epic-001 README](../../../product/epics/epic-001-behavior-constraints/README.md)

---

**æœ€åæ›´æ–°**: 2025-10-27
**æ›´æ–°äºº**: EvolvAI Team
**å‰ç½®æ¡ä»¶**: Story 0.1 + Story 0.2 å®Œæˆ
**äº¤ä»˜**: Phase 0 å®Œæˆï¼Œå‡†å¤‡è¿›å…¥ Epic-001 Phase 1
