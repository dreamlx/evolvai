# Story 0.1 TDD å¼€å‘è®¡åˆ’ï¼šå®ç° ToolExecutionEngine

**Story ID**: STORY-0.1
**Epic**: Epic-001 (Phase 0: å·¥å…·è°ƒç”¨é“¾è·¯ç®€åŒ–)
**å·¥æœŸ**: 5 äººå¤©
**é£é™©**: ğŸŸ¡ ä¸­ç­‰
**çŠ¶æ€**: [PLANNING]

---

## ğŸ“‹ Story ç›®æ ‡

å®ç°ç»Ÿä¸€çš„ ToolExecutionEngineï¼Œä½œä¸ºæ‰€æœ‰å·¥å…·æ‰§è¡Œçš„å”¯ä¸€å…¥å£ï¼Œæä¾›å®Œæ•´çš„å®¡è®¡èƒ½åŠ›å’Œ TPST åˆ†æåŸºç¡€ã€‚

**äº¤ä»˜ç‰©**ï¼š
1. `ExecutionPhase` æšä¸¾ - æ‰§è¡Œé˜¶æ®µå®šä¹‰
2. `ExecutionContext` æ•°æ®ç±» - å®Œæ•´å®¡è®¡ä¿¡æ¯
3. `ToolExecutionEngine` ç±» - 4 é˜¶æ®µæ‰§è¡Œæµç¨‹
4. å®¡è®¡æ—¥å¿—æ¥å£ - è®°å½•å’ŒæŸ¥è¯¢æ‰§è¡Œå†å²
5. TPST åˆ†ææ¥å£ - token æ¶ˆè€—åˆ†æ

---

## ğŸ§ª TDD å¼€å‘ç­–ç•¥

### Red-Green-Refactor å¾ªç¯è®¡åˆ’

é‡‡ç”¨**å‚ç›´åˆ‡ç‰‡**ç­–ç•¥ï¼Œæ¯ä¸ªå¾ªç¯å®Œæˆä¸€ä¸ªå®Œæ•´çš„åŠŸèƒ½åˆ‡ç‰‡ï¼š

```
Cycle 1: ExecutionPhase + ExecutionContext (åŸºç¡€æ•°æ®ç»“æ„)
  Red â†’ Green â†’ Refactor â†’ Commit

Cycle 2: ToolExecutionEngine æ ¸å¿ƒæ¡†æ¶ (4 é˜¶æ®µæµç¨‹)
  Red â†’ Green â†’ Refactor â†’ Commit

Cycle 3: Pre-validation é˜¶æ®µå®ç°
  Red â†’ Green â†’ Refactor â†’ Commit

Cycle 4: Execution é˜¶æ®µå®ç°
  Red â†’ Green â†’ Refactor â†’ Commit

Cycle 5: Post-execution é˜¶æ®µå®ç°
  Red â†’ Green â†’ Refactor â†’ Commit

Cycle 6: å®¡è®¡æ—¥å¿—æ¥å£
  Red â†’ Green â†’ Refactor â†’ Commit

Cycle 7: TPST åˆ†ææ¥å£
  Red â†’ Green â†’ Refactor â†’ Commit
```

---

## ğŸ“ æµ‹è¯•æ–‡ä»¶ç»“æ„

```
test/evolvai/
â”œâ”€â”€ __init__.py
â””â”€â”€ core/
    â”œâ”€â”€ __init__.py
    â”œâ”€â”€ test_execution_context.py      # ExecutionContext æµ‹è¯•
    â”œâ”€â”€ test_execution_engine_core.py  # ToolExecutionEngine æ ¸å¿ƒæµ‹è¯•
    â”œâ”€â”€ test_execution_phases.py       # å„é˜¶æ®µåŠŸèƒ½æµ‹è¯•
    â”œâ”€â”€ test_audit_log.py              # å®¡è®¡æ—¥å¿—æµ‹è¯•
    â””â”€â”€ test_tpst_analysis.py          # TPST åˆ†ææµ‹è¯•

test/conftest.py                        # æ·»åŠ æ–°çš„ fixtures
```

---

## ğŸ”´ Cycle 1: åŸºç¡€æ•°æ®ç»“æ„

### Red é˜¶æ®µ - ç¼–å†™æµ‹è¯•

**æµ‹è¯•æ–‡ä»¶**: `test/evolvai/core/test_execution_context.py`

```python
"""Tests for ExecutionContext and ExecutionPhase."""
import time
from enum import Enum

import pytest

from evolvai.core.execution import ExecutionContext, ExecutionPhase


class TestExecutionPhase:
    """Test ExecutionPhase enum."""

    def test_execution_phase_values(self):
        """Test that all expected phases exist."""
        assert ExecutionPhase.PRE_VALIDATION
        assert ExecutionPhase.PRE_EXECUTION
        assert ExecutionPhase.EXECUTION
        assert ExecutionPhase.POST_EXECUTION

    def test_execution_phase_ordering(self):
        """Test that phases have correct ordering."""
        phases = list(ExecutionPhase)
        assert phases[0] == ExecutionPhase.PRE_VALIDATION
        assert phases[1] == ExecutionPhase.PRE_EXECUTION
        assert phases[2] == ExecutionPhase.EXECUTION
        assert phases[3] == ExecutionPhase.POST_EXECUTION


class TestExecutionContext:
    """Test ExecutionContext dataclass."""

    def test_execution_context_creation(self):
        """Test basic ExecutionContext creation."""
        ctx = ExecutionContext(
            tool_name="test_tool",
            kwargs={"arg1": "value1"},
        )

        assert ctx.tool_name == "test_tool"
        assert ctx.kwargs == {"arg1": "value1"}
        assert ctx.execution_plan is None
        assert ctx.phase == ExecutionPhase.PRE_VALIDATION
        assert ctx.result is None
        assert ctx.error is None

    def test_execution_context_with_plan(self):
        """Test ExecutionContext with execution plan."""
        plan = {"steps": ["step1", "step2"]}
        ctx = ExecutionContext(
            tool_name="test_tool",
            kwargs={},
            execution_plan=plan
        )

        assert ctx.execution_plan == plan

    def test_execution_context_timing(self):
        """Test ExecutionContext time tracking."""
        ctx = ExecutionContext(
            tool_name="test_tool",
            kwargs={},
        )

        ctx.start_time = time.time()
        time.sleep(0.01)  # Small delay
        ctx.end_time = time.time()

        duration = ctx.end_time - ctx.start_time
        assert duration > 0
        assert duration < 0.1  # Should be quick

    def test_execution_context_token_tracking(self):
        """Test ExecutionContext token metrics."""
        ctx = ExecutionContext(
            tool_name="test_tool",
            kwargs={},
        )

        ctx.estimated_tokens = 100
        ctx.actual_tokens = 95

        assert ctx.estimated_tokens == 100
        assert ctx.actual_tokens == 95

    def test_execution_context_constraint_tracking(self):
        """Test ExecutionContext constraint violation tracking."""
        ctx = ExecutionContext(
            tool_name="test_tool",
            kwargs={},
        )

        ctx.constraint_violations = ["violation1", "violation2"]
        ctx.should_batch = True

        assert len(ctx.constraint_violations) == 2
        assert ctx.should_batch is True

    def test_execution_context_to_audit_record(self):
        """Test conversion to audit record."""
        ctx = ExecutionContext(
            tool_name="test_tool",
            kwargs={"arg": "value"},
        )

        ctx.start_time = time.time()
        ctx.end_time = ctx.start_time + 0.5
        ctx.phase = ExecutionPhase.EXECUTION
        ctx.actual_tokens = 100
        ctx.constraint_violations = []
        ctx.should_batch = False

        record = ctx.to_audit_record()

        assert record["tool"] == "test_tool"
        assert record["phase"] == "execution"
        assert record["duration"] == 0.5
        assert record["tokens"] == 100
        assert record["success"] is True
        assert record["constraints"] == []
        assert record["batched"] is False

    def test_execution_context_to_audit_record_with_error(self):
        """Test audit record with error."""
        ctx = ExecutionContext(
            tool_name="test_tool",
            kwargs={},
        )

        ctx.start_time = time.time()
        ctx.end_time = time.time()
        ctx.error = ValueError("test error")

        record = ctx.to_audit_record()

        assert record["success"] is False
```

### Green é˜¶æ®µ - æœ€å°å®ç°

**å®ç°æ–‡ä»¶**: `src/evolvai/core/execution.py`

```python
"""Core execution engine components."""
from dataclasses import dataclass, field
from enum import Enum
from typing import Any


class ExecutionPhase(Enum):
    """Execution phases for tool execution."""

    PRE_VALIDATION = "pre_validation"
    PRE_EXECUTION = "pre_execution"
    EXECUTION = "execution"
    POST_EXECUTION = "post_execution"


@dataclass
class ExecutionContext:
    """Complete execution context with audit trail."""

    # Tool information
    tool_name: str
    kwargs: dict[str, Any]
    execution_plan: Any | None = None

    # Timing
    start_time: float = 0.0
    end_time: float = 0.0
    phase: ExecutionPhase = ExecutionPhase.PRE_VALIDATION

    # Constraint tracking (Epic-001)
    constraint_violations: list[str] | None = None
    should_batch: bool = False

    # Execution results
    result: str | None = None
    error: Exception | None = None

    # Token tracking (TPST core)
    estimated_tokens: int = 0
    actual_tokens: int = 0

    def to_audit_record(self) -> dict[str, Any]:
        """Convert to audit record for TPST analysis."""
        return {
            "tool": self.tool_name,
            "phase": self.phase.value,
            "duration": self.end_time - self.start_time,
            "tokens": self.actual_tokens,
            "success": self.error is None,
            "constraints": self.constraint_violations or [],
            "batched": self.should_batch,
        }
```

### Refactor é˜¶æ®µ

- æ£€æŸ¥ç±»å‹æ³¨è§£å®Œæ•´æ€§
- æ·»åŠ  docstring
- è¿è¡Œ `uv run poe format`
- è¿è¡Œ `uv run poe type-check`

---

## ğŸ”´ Cycle 2: ToolExecutionEngine æ ¸å¿ƒæ¡†æ¶

### Red é˜¶æ®µ - ç¼–å†™æµ‹è¯•

**æµ‹è¯•æ–‡ä»¶**: `test/evolvai/core/test_execution_engine_core.py`

```python
"""Tests for ToolExecutionEngine core functionality."""
from unittest.mock import Mock, MagicMock

import pytest

from evolvai.core.execution import ExecutionContext, ExecutionPhase, ToolExecutionEngine
from serena.tools.tools_base import Tool


class MockTool(Tool):
    """Mock tool for testing."""

    def apply(self, test_arg: str) -> str:
        """Mock apply method.

        :param test_arg: Test argument
        :return: Test result
        """
        return f"result: {test_arg}"


class TestToolExecutionEngine:
    """Test ToolExecutionEngine core functionality."""

    @pytest.fixture
    def mock_agent(self):
        """Create mock SerenaAgent."""
        agent = Mock()
        agent.serena_config = Mock()
        agent.serena_config.tool_timeout = 60
        agent._active_project = Mock()
        agent.is_using_language_server = Mock(return_value=False)
        return agent

    @pytest.fixture
    def mock_tool(self, mock_agent):
        """Create mock tool."""
        return MockTool(mock_agent)

    @pytest.fixture
    def engine(self, mock_agent):
        """Create ToolExecutionEngine instance."""
        return ToolExecutionEngine(agent=mock_agent)

    def test_engine_initialization(self, engine):
        """Test engine initialization."""
        assert engine is not None
        assert engine._constraints_enabled is False
        assert engine._audit_log == []

    def test_engine_with_constraints_enabled(self, mock_agent):
        """Test engine with constraints enabled."""
        engine = ToolExecutionEngine(agent=mock_agent, enable_constraints=True)
        assert engine._constraints_enabled is True

    def test_execute_basic_tool(self, engine, mock_tool):
        """Test basic tool execution."""
        result = engine.execute(mock_tool, test_arg="hello")

        assert result == "result: hello"
        assert len(engine._audit_log) == 1

    def test_execute_creates_context(self, engine, mock_tool):
        """Test that execution creates proper context."""
        result = engine.execute(mock_tool, test_arg="test")

        audit_record = engine._audit_log[0]
        assert audit_record["tool"] == "mock"
        assert audit_record["success"] is True

    def test_execute_tracks_phases(self, engine, mock_tool):
        """Test that execution goes through all phases."""
        # This will be tested via audit log
        result = engine.execute(mock_tool, test_arg="test")

        # Should have gone through all phases successfully
        assert result == "result: test"

    def test_execute_handles_exceptions(self, engine, mock_agent):
        """Test exception handling during execution."""
        class ErrorTool(Tool):
            def apply(self) -> str:
                """Apply method that raises."""
                raise ValueError("Test error")

        error_tool = ErrorTool(mock_agent)
        result = engine.execute(error_tool)

        # Should catch exception and return error message
        assert "Error" in result or "error" in result.lower()

        # Audit log should show failure
        audit_record = engine._audit_log[0]
        assert audit_record["success"] is False

    def test_execute_with_execution_plan(self, engine, mock_tool):
        """Test execution with ExecutionPlan."""
        plan = {"steps": ["analyze", "execute"]}
        result = engine.execute(mock_tool, execution_plan=plan, test_arg="test")

        assert result == "result: test"
        # Plan should be in audit record
        # (Will verify this in integration tests)
```

### Green é˜¶æ®µ - æœ€å°å®ç°

ç»§ç»­åœ¨ `src/evolvai/core/execution.py` ä¸­æ·»åŠ ï¼š

```python
import time
from typing import TYPE_CHECKING

from sensai.util import logging

if TYPE_CHECKING:
    from serena.agent import SerenaAgent
    from serena.tools.tools_base import Tool

log = logging.getLogger(__name__)


class ToolExecutionEngine:
    """Unified tool execution engine.

    Provides:
    1. 4-phase execution flow
    2. Complete audit trail
    3. Epic-001 constraint integration point
    4. TPST analysis support
    """

    def __init__(self, agent: "SerenaAgent", enable_constraints: bool = False):
        """Initialize execution engine.

        :param agent: SerenaAgent instance
        :param enable_constraints: Enable Epic-001 constraints
        """
        self._agent = agent
        self._constraints_enabled = enable_constraints
        self._audit_log: list[dict[str, Any]] = []

    def execute(self, tool: "Tool", **kwargs) -> str:
        """Execute tool with full 4-phase flow.

        :param tool: Tool to execute
        :param kwargs: Tool arguments
        :return: Tool execution result
        """
        # Extract execution_plan if present
        execution_plan = kwargs.pop("execution_plan", None)

        # Create execution context
        ctx = ExecutionContext(
            tool_name=tool.get_name(),
            kwargs=kwargs.copy(),
            execution_plan=execution_plan,
            start_time=time.time()
        )

        try:
            # Phase 1: Pre-validation
            ctx.phase = ExecutionPhase.PRE_VALIDATION
            self._pre_validation(tool, ctx)

            # Phase 2: Pre-execution (Epic-001)
            if self._constraints_enabled:
                ctx.phase = ExecutionPhase.PRE_EXECUTION
                self._pre_execution_with_constraints(tool, ctx)

            # Phase 3: Execution
            ctx.phase = ExecutionPhase.EXECUTION
            ctx.result = self._execute_tool(tool, ctx)

            # Phase 4: Post-execution
            ctx.phase = ExecutionPhase.POST_EXECUTION
            self._post_execution(tool, ctx)

            return ctx.result

        except Exception as e:
            ctx.error = e
            log.error(f"Error executing tool {tool.get_name()}: {e}", exc_info=e)
            return f"Error executing tool: {e}"

        finally:
            ctx.end_time = time.time()
            self._audit_log.append(ctx.to_audit_record())

    def _pre_validation(self, tool: "Tool", ctx: ExecutionContext) -> None:
        """Phase 1: Pre-validation checks."""
        # Will implement in Cycle 3
        pass

    def _pre_execution_with_constraints(self, tool: "Tool", ctx: ExecutionContext) -> None:
        """Phase 2: Pre-execution with constraints (Epic-001)."""
        # Placeholder for Epic-001 integration
        pass

    def _execute_tool(self, tool: "Tool", ctx: ExecutionContext) -> str:
        """Phase 3: Actual tool execution."""
        apply_fn = tool.get_apply_fn()
        result = apply_fn(**ctx.kwargs)
        return result

    def _post_execution(self, tool: "Tool", ctx: ExecutionContext) -> None:
        """Phase 4: Post-execution cleanup."""
        # Will implement in Cycle 5
        pass
```

---

## ğŸ”´ Cycle 3-7: åç»­å¾ªç¯

åç»­å¾ªç¯éµå¾ªç›¸åŒçš„ Red-Green-Refactor æ¨¡å¼ï¼š

### Cycle 3: Pre-validation é˜¶æ®µ
- æµ‹è¯•ï¼šå·¥å…·æ¿€æ´»æ£€æŸ¥ã€é¡¹ç›®æ£€æŸ¥ã€LSP æ£€æŸ¥
- å®ç°ï¼š`_pre_validation()` å®Œæ•´é€»è¾‘

### Cycle 4: Execution é˜¶æ®µä¼˜åŒ–
- æµ‹è¯•ï¼šçº¿ç¨‹æ± é›†æˆã€è¶…æ—¶å¤„ç†ã€token ä¼°ç®—
- å®ç°ï¼šå¼‚æ­¥æ‰§è¡Œã€èµ„æºç®¡ç†

### Cycle 5: Post-execution é˜¶æ®µ
- æµ‹è¯•ï¼šæ—¥å¿—è®°å½•ã€ç»Ÿè®¡ä¸ŠæŠ¥ã€ç¼“å­˜ä¿å­˜
- å®ç°ï¼š`_post_execution()` å®Œæ•´é€»è¾‘

### Cycle 6: å®¡è®¡æ—¥å¿—æ¥å£
- æµ‹è¯•ï¼šæ—¥å¿—æŸ¥è¯¢ã€è¿‡æ»¤ã€å¯¼å‡º
- å®ç°ï¼š`get_audit_log()`, `clear_audit_log()` ç­‰æ–¹æ³•

### Cycle 7: TPST åˆ†ææ¥å£
- æµ‹è¯•ï¼štoken ç»Ÿè®¡ã€æ€§èƒ½åˆ†æã€ä¼˜åŒ–å»ºè®®
- å®ç°ï¼š`analyze_tpst()`, `get_slow_tools()` ç­‰æ–¹æ³•

---

## ğŸ§° Mock å’Œ Fixture ç­–ç•¥

### å…±äº« Fixtures (åœ¨ conftest.py ä¸­)

```python
@pytest.fixture
def mock_serena_agent():
    """Create mock SerenaAgent for testing."""
    agent = Mock()
    agent.serena_config = Mock()
    agent.serena_config.tool_timeout = 60
    agent.serena_config.default_max_tool_answer_chars = 10000
    agent._active_project = Mock()
    agent.is_using_language_server = Mock(return_value=False)
    agent.is_language_server_running = Mock(return_value=False)
    agent.tool_is_active = Mock(return_value=True)
    agent.get_active_tool_names = Mock(return_value=["mock_tool"])
    return agent

@pytest.fixture
def execution_engine(mock_serena_agent):
    """Create ToolExecutionEngine for testing."""
    from evolvai.core.execution import ToolExecutionEngine
    return ToolExecutionEngine(agent=mock_serena_agent)

@pytest.fixture
def execution_engine_with_constraints(mock_serena_agent):
    """Create ToolExecutionEngine with constraints enabled."""
    from evolvai.core.execution import ToolExecutionEngine
    return ToolExecutionEngine(agent=mock_serena_agent, enable_constraints=True)
```

---

## âœ… Definition of Done

### ä»£ç è´¨é‡

- [ ] æ‰€æœ‰æµ‹è¯•é€šè¿‡ (`uv run poe test test/evolvai/core`)
- [ ] æµ‹è¯•è¦†ç›–ç‡ â‰¥ 90% (`pytest --cov=evolvai.core`)
- [ ] ç±»å‹æ£€æŸ¥é€šè¿‡ (`uv run poe type-check`)
- [ ] ä»£ç æ ¼å¼åŒ–é€šè¿‡ (`uv run poe format`)
- [ ] æ—  lint è­¦å‘Š (`uv run poe lint`)

### åŠŸèƒ½å®Œæ•´æ€§

- [ ] ExecutionPhase æšä¸¾å®šä¹‰å®Œæ•´
- [ ] ExecutionContext æ•°æ®ç±»åŠŸèƒ½å®Œæ•´
- [ ] ToolExecutionEngine 4 é˜¶æ®µæµç¨‹æ­£å¸¸å·¥ä½œ
- [ ] å®¡è®¡æ—¥å¿—æ­£ç¡®è®°å½•æ‰€æœ‰æ‰§è¡Œ
- [ ] TPST åˆ†ææ¥å£å¯ç”¨

### æ–‡æ¡£

- [ ] æ‰€æœ‰ç±»å’Œæ–¹æ³•æœ‰å®Œæ•´çš„ docstring
- [ ] å…³é”®è®¾è®¡å†³ç­–æœ‰æ³¨é‡Šè¯´æ˜
- [ ] README æˆ–è®¾è®¡æ–‡æ¡£æ›´æ–°

### é›†æˆ

- [ ] ä¸ç°æœ‰ Tool ç³»ç»Ÿå…¼å®¹
- [ ] Mock SerenaAgent ä¾èµ–æ¸…æ™°
- [ ] ä¸º Story 0.2 é›†æˆåšå¥½å‡†å¤‡

---

## ğŸ“Š æ¯æ—¥è¿›åº¦è·Ÿè¸ª

### Day 1: Cycle 1-2 (åŸºç¡€æ¡†æ¶)
- âœ… Cycle 1: ExecutionPhase + ExecutionContext
- âœ… Cycle 2: ToolExecutionEngine æ ¸å¿ƒ

### Day 2: Cycle 3-4 (æ ¸å¿ƒé˜¶æ®µ)
- â³ Cycle 3: Pre-validation å®ç°
- â³ Cycle 4: Execution é˜¶æ®µä¼˜åŒ–

### Day 3: Cycle 5 (å®Œå–„é˜¶æ®µ)
- â³ Cycle 5: Post-execution å®ç°

### Day 4: Cycle 6-7 (åˆ†ææ¥å£)
- â³ Cycle 6: å®¡è®¡æ—¥å¿—æ¥å£
- â³ Cycle 7: TPST åˆ†ææ¥å£

### Day 5: å®Œå–„å’Œæµ‹è¯•
- â³ é›†æˆæµ‹è¯•
- â³ æ€§èƒ½æµ‹è¯•
- â³ æ–‡æ¡£å®Œå–„
- â³ DoD æ£€æŸ¥

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [ADR-003: å·¥å…·è°ƒç”¨é“¾è·¯ç®€åŒ–](../../architecture/adrs/003-tool-execution-engine-simplification.md)
- [Phase 0 è¯¦ç»†è®¾è®¡](../../architecture/phase-0-tool-execution-engine.md)
- [Epic-001 README](../../../product/epics/epic-001-behavior-constraints/README.md)

---

**æœ€åæ›´æ–°**: 2025-10-27
**æ›´æ–°äºº**: EvolvAI Team
**ä¸‹ä¸€æ­¥**: å¼€å§‹ Day 1 - Cycle 1 çš„ Red é˜¶æ®µ
